{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.utils as utils\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, corpus_length = None, device = None):\n",
    "        corpus_file = open('./data/train_shuf.txt')\n",
    "\n",
    "        if device == None:\n",
    "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        if corpus_length == None:\n",
    "            corpus_length = sum(1 for line in corpus_file)\n",
    "        \n",
    "        self.corpus = []\n",
    "\n",
    "        for i in tqdm(range(corpus_length)):\n",
    "            self.corpus.append(utils.simple_preprocess(corpus_file.readline(), min_len=1))\n",
    "            # self.corpus.append(word_tokenize(corpus_file.readline().lower()))\n",
    "\n",
    "        self.corpus = sorted(self.corpus, key=lambda x: len(x))\n",
    "        \n",
    "        self.unique_words = self.get_unique_words()\n",
    "\n",
    "        self.index_to_word = {index: word for index, word in enumerate(self.unique_words)}\n",
    "        self.word_to_index = {word: index for index, word in enumerate(self.unique_words)}\n",
    "\n",
    "        self.input_corpus_indexes = [list(map(lambda word: self.word_to_index[word], sentence)) for sentence in self.corpus]\n",
    "        output_corpus = [sentence[1:] + ['<STOP>'] for sentence in self.corpus]\n",
    "\n",
    "        self.output_corpus_indexes = [list(map(lambda word: self.word_to_index[word], sentence)) for sentence in output_corpus]\n",
    "        \n",
    "        self.device = device\n",
    "\n",
    "\n",
    "    def indexes_to_sentence(self, sentence):\n",
    "        return list(map(lambda x: self.index_to_word[x], sentence))\n",
    "\n",
    "\n",
    "    def get_unique_words(self):\n",
    "        words = list(set([word for line in self.corpus for word in line]))\n",
    "        words.sort()\n",
    "        words = ['<PAD>', '<STOP>'] + words\n",
    "        self.pad_index = 0\n",
    "        self.stop_index = 1\n",
    "        return words\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.corpus)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (torch.tensor(self.input_corpus_indexes[index], device=self.device),\n",
    "            torch.tensor(self.output_corpus_indexes[index], device=self.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_collate(data):\n",
    "    def left_pad_sequence(tensors):\n",
    "        max_len = max(list(map(len, tensors)))\n",
    "        padded_seq = [torch.hstack([torch.zeros(max_len - len(t), device=t.device, dtype=torch.int32), t]) for t in tensors]\n",
    "        return torch.stack(padded_seq)\n",
    "\n",
    "\n",
    "    inputs = [d[0] for d in data]\n",
    "    outputs = [d[1] for d in data]\n",
    "    inputs = left_pad_sequence(inputs)\n",
    "    outputs = left_pad_sequence(outputs)\n",
    "    return inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 19872.57it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset(corpus_length = 100, device=device)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=5, collate_fn=pad_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<PAD>', '<PAD>', 'wstrzymał', 'się', 'od', 'głosu', '<STOP>']\n",
      "['<PAD>', 'z', 'pomidorami', 'bazylią', 'i', 'serem', '<STOP>']\n",
      "['<PAD>', 'pośle', 'czy', 'w', 'sprawie', 'formalnej', '<STOP>']\n",
      "['ust', 'ustawy', 'z', 'dnia', 'sierpnia', 'r', '<STOP>']\n",
      "['stanowiły', 'sumy', 'aktywów', 'trwałych', 'i', 'obrotowych', '<STOP>']\n",
      "-----------------------------------\n",
      "['<PAD>', 'cylindryczny', 'korpus', 'został', 'zwieńczony', 'stożkowym', 'hełmem', '<STOP>']\n",
      "['euroraty', 'chcesz', 'kupować', 'więcej', 'niż', 'gdzie', 'indziej', '<STOP>']\n",
      "['to', 'ponieważ', 'pan', 'poseł', 'pytał', 'o', 'finansowanie', '<STOP>']\n",
      "['z', 'powodu', 'wygłodzenia', 'i', 'chorób', 'sięgała', 'miesięcznie', '<STOP>']\n",
      "['syn', 'adam', 'mandziara', 'jest', 'znanym', 'menedżerem', 'piłkarskim', '<STOP>']\n",
      "-----------------------------------\n",
      "['<PAD>', 'został', 'ukończony', 'sierpnia', 'i', 'września', 'przekazany', 'armatorowi', '<STOP>']\n",
      "['ten', 'przypadek', 'odstraszy', 'innych', 'kłusowników', 'od', 'tego', 'procederu', '<STOP>']\n",
      "['w', 'ameryce', 'południowej', 'gdzie', 'odgrywa', 'ważną', 'rolę', 'ekologiczną', '<STOP>']\n",
      "['na', 'ukrainie', 'blisko', 'seminariów', 'biblijnych', 'instytutów', 'oraz', 'szkół', '<STOP>']\n",
      "['world', 'of', 'tanks', 'jgpze', 'equipment', 'to', 'fantastyczne', 'prezenty', '<STOP>']\n",
      "-----------------------------------\n",
      "['<PAD>', 'ciekawych', 'technicznych', 'odcinkach', 'specjalnych', 'team', 'spisał', 'się', 'znakomicie', '<STOP>']\n",
      "['<PAD>', 'bedzie', 'slownikow', 'nie', 'bedzie', 'namierzania', 'nie', 'bedzie', 'nic', '<STOP>']\n",
      "['<PAD>', 'nasz', 'projekt', 'dotyczył', 'wszystkich', 'dzieci', 'narodzonych', 'w', 'gdańsku', '<STOP>']\n",
      "['możesz', 'wydłużyć', 'ważność', 'swojego', 'konta', 'na', 'połączenia', 'wychodzące', 'do', '<STOP>']\n",
      "['absolwentów', 'szkół', 'powiatu', 'nowomiejskiego', 'zdających', 'egzamin', 'maturalny', 'w', 'r', '<STOP>']\n",
      "-----------------------------------\n",
      "['rolach', 'głównych', 'wystąpili', 'jake', 'gyllenhaal', 'forest', 'whitaker', 'oraz', 'rachel', 'mcadams', '<STOP>']\n",
      "['więc', 'sobie', 'oszczędzić', 'trochę', 'czasu', 'i', 'pieniędzy', 'dzięki', 'wykonaniu', 'lustracji', '<STOP>']\n",
      "['wpłaty', 'zadatku', 'wiąże', 'się', 'z', 'anulowaniem', 'wstępnej', 'rezerwacji', 'bez', 'potwierdzenia', '<STOP>']\n",
      "['o', 'zabranie', 'głosu', 'panią', 'poseł', 'beatę', 'mazurek', 'prawo', 'i', 'sprawiedliwość', '<STOP>']\n",
      "['skutecznym', 'urządzeniem', 'które', 'pozwoli', 'ci', 'przekopać', 'się', 'na', 'sporą', 'głębokość', '<STOP>']\n",
      "-----------------------------------\n",
      "['<PAD>', 'taniość', 'i', 'wypoczynek', 'niekoniecznie', 'na', 'plaży', 'polska', 'nie', 'ma', 'konkurencji', '<STOP>']\n",
      "['czy', 'bedzie', 'bezposredni', 'przeszczep', 'z', 'asx', 'a', 'czy', 'moze', 'cos', 'nowego', '<STOP>']\n",
      "['została', 'zniesiona', 'września', 'wraz', 'z', 'reformą', 'wprowadzającą', 'gromady', 'w', 'miejsce', 'gmin', '<STOP>']\n",
      "['lkl', 'services', 'for', 'agriculture', 'poszukuje', 'osób', 'chętnych', 'do', 'pracy', 'na', 'stanowisku', '<STOP>']\n",
      "['proszę', 'o', 'zabranie', 'głosu', 'pana', 'posła', 'antoniego', 'błądka', 'prawo', 'i', 'sprawiedliwość', '<STOP>']\n",
      "-----------------------------------\n",
      "['<PAD>', 'takiej', 'zmiany', 'na', 'stałe', 'dałoby', 'wymierne', 'korzyści', 'przede', 'wszystkim', 'dla', 'dzieci', '<STOP>']\n",
      "['kogoś', 'w', 'polsce', 'do', 'hitlera', 'jest', 'najbardziej', 'haniebną', 'kalumnią', 'jaką', 'można', 'rzucić', '<STOP>']\n",
      "['this', 'is', 'next', 'time', 'na', 'amerykańskim', 'rynku', 'muzycznym', 'ukazała', 'się', 'w', 'roku', '<STOP>']\n",
      "['tam', 'jest', 'naprawdę', 'utalentowany', 'limit', 'jako', 'gracza', 'dziękujemy', 'thunder', 'valley', 'kasyna', 'głównego', '<STOP>']\n",
      "['służą', 'także', 'do', 'zarządzania', 'bieżącymi', 'zagadnieniami', 'firmy', 'na', 'różnym', 'poziomie', 'jednostek', 'organizacyjnych', '<STOP>']\n",
      "-----------------------------------\n",
      "['<PAD>', 'zdecydował', 'jednak', 'inaczej', 'i', 'przyjął', 'w', 'ustawie', 'z', 'dnia', 'r', 'postępowanie', 'orzeczniczo', 'lekarskie', '<STOP>']\n",
      "['<PAD>', 'samo', 'muszą', 'wzrastać', 'lokaty', 'bankowe', 'ze', 'względu', 'na', 'wzrost', 'inflacji', 'z', 'których', 'banki', '<STOP>']\n",
      "['<PAD>', 'się', 'w', 'niej', 'typowe', 'dla', 'gór', 'piętra', 'regiel', 'piętro', 'kosodrzewiny', 'i', 'piętro', 'halne', '<STOP>']\n",
      "['<PAD>', 'obecnej', 'sytuacji', 'pani', 'p', 'nie', 'jest', 'w', 'posiadaniu', 'wystarczającej', 'kwoty', 'na', 'pokrycie', 'grzywny', '<STOP>']\n",
      "['możecie', 'zobaczyć', 'jak', 'wygląda', 'to', 'w', 'praktyce', 'na', 'mapie', 'są', 'wyszczególnione', 'miejsca', 'objęte', 'programem', '<STOP>']\n",
      "-----------------------------------\n",
      "['się', 'również', 'klimat', 'całej', 'planety', 'ponieważ', 'lasy', 'tropikalne', 'są', 'ważnym', 'ogniwem', 'obiegu', 'wielu', 'pierwiastków', '<STOP>']\n",
      "['andrzejewski', 'dowódca', 'kompanii', 'strzeleckiej', 'por', 'jan', 'i', 'wojciechowski', 'od', 'ix', 'dowódca', 'kompanii', 'strzeleckiej', 'ppor', '<STOP>']\n",
      "['kwaterze', 'głównej', 'marszałka', 'polnego', 'von', 'weichsa', 'dowódcy', 'grupy', 'armii', 'b', 'coraz', 'bardziej', 'narastało', 'zwątpienie', '<STOP>']\n",
      "['zostałeś', 'poddany', 'żadnej', 'próbie', 'więc', 'proszę', 'o', 'odrobinę', 'pokory', 'wobec', 'historii', 'i', 'jej', 'bohaterów', '<STOP>']\n",
      "['porzucił', 'pracę', 'księgowego', 'w', 'i', 'utworzył', 'swoją', 'pierwszą', 'klinikę', 'easyway', 'aby', 'pomagać', 'innym', 'palaczom', '<STOP>']\n",
      "-----------------------------------\n",
      "['<PAD>', 'w', 'wojnie', 'obronnej', 'polski', 'w', 'w', 'wyniku', 'czego', 'został', 'wywieziony', 'do', 'obozu', 'w', 'rzeszy', '<STOP>']\n",
      "['<PAD>', 'zdarzenia', 'sfotografowali', 'sprawcę', 'podczas', 'gwałtu', 'i', 'powiadomili', 'policję', 'która', 'schwytała', 'go', 'kilka', 'godzin', 'później', '<STOP>']\n",
      "['<PAD>', 'powiedział', 'że', 'możliwe', 'że', 'mam', 'jeszcze', 'stulejkę', 'ale', 'okaże', 'się', 'dopiero', 'po', 'jakichś', 'tygodniach', '<STOP>']\n",
      "['<PAD>', 'instytucją', 'bankową', 'w', 'polsce', 'działającą', 'na', 'zasadach', 'współdziałania', 'i', 'samopomocy', 'była', 'założona', 'w', 'r', '<STOP>']\n",
      "['i', 'przywileje', 'są', 'w', 'stosunkach', 'między', 'państwami', 'zwykle', 'wzajemne', 'ambasador', 'ma', 'tu', 'niewielkie', 'pole', 'działania', '<STOP>']\n",
      "-----------------------------------\n",
      "['wzgórz', 'do', 'strony', 'południowej', 'do', 'państwa', 'wpływał', 'wierzbiak', 'który', 'znany', 'był', 'już', 'ze', 'złotonośnych', 'piasków', '<STOP>']\n",
      "['nie', 'zmniejszyło', 'per', 'saldo', 'finansowania', 'szkolnictwa', 'zawodowego', 'jedynie', 'doprecyzowało', 'finansowanie', 'zgodnie', 'z', 'istniejącym', 'stanem', 'faktycznym', '<STOP>']\n",
      "['jest', 'nie', 'to', 'że', 'produkt', 'nazywa', 'się', 'sok', 'ale', 'co', 'jest', 'napisane', 'na', 'jego', 'opakowaniu', '<STOP>']\n",
      "['roku', 'zdał', 'egzamin', 'na', 'licencję', 'zawodnika', 'w', 'zkż', 'zielona', 'góra', 'gdzie', 'startował', 'do', 'końca', 'sezonu', '<STOP>']\n",
      "['edss', 'ustabilizowała', 'się', 'u', 'chorych', 'zmniejszyła', 'się', 'u', 'a', 'u', 'zwiększyła', 'się', 'z', 'czego', 'zmarło', '<STOP>']\n",
      "-----------------------------------\n",
      "['tę', 'pozbawiono', 'również', 'gazu', 'który', 'był', 'magazynowany', 'w', 'ukraińskich', 'pmg', 'i', 'zbywany', 'm', 'in', 'do', 'polski', '<STOP>']\n",
      "['poziom', 'wskaźnika', 'spowodował', 'zmniejszenie', 'liczby', 'uczniów', 'o', 'uczniów', 'co', 'wpłynęło', 'na', 'zmniejszenie', 'kwoty', 'subwencji', 'o', 'zł', '<STOP>']\n",
      "['z', 'halikarnasu', 'w', 'uwagach', 'o', 'starożytnych', 'oratorach', 'wymienia', 'mów', 'przypisywanych', 'dejnarchosowi', 'z', 'tego', 'uznaje', 'za', 'autentyczne', '<STOP>']\n",
      "['rzymian', 'podobnie', 'phalerae', 'wkrótce', 'stały', 'się', 'rodzajem', 'odznaczenia', 'za', 'męstwo', 'przyznawanego', 'konkretnym', 'żołnierzom', 'lub', 'całym', 'jednostkom', '<STOP>']\n",
      "['akceleratorach', 'liniowych', 'redukowane', 'są', 'straty', 'energii', 'związane', 'z', 'zakrzywieniem', 'toru', 'cząstek', 'naładowanych', 'występujące', 'w', 'akceleratorach', 'kołowych', '<STOP>']\n",
      "-----------------------------------\n",
      "['pytanie', 'czy', 'mamy', 'zastąpić', 'obecny', 'podatek', 'od', 'nieruchomości', 'inną', 'formą', 'tego', 'podatku', 'podatkiem', 'od', 'wartości', 'nieruchomości', '<STOP>']\n",
      "['że', 'sejm', 'odrzuca', 'poprawkę', 'senatu', 'bezwzględną', 'większością', 'głosów', 'w', 'obecności', 'co', 'najmniej', 'połowy', 'ustawowej', 'liczby', 'posłów', '<STOP>']\n",
      "['się', 'że', 'polonia', 'jest', 'w', 'naturalny', 'sposób', 'zainteresowana', 'bardziej', 'efektywną', 'promocją', 'ojczystego', 'kraju', 'w', 'opinii', 'międzynarodowej', '<STOP>']\n",
      "['władca', 'zwrócony', 'jest', 'w', 'prawą', 'stronę', 'w', 'kierunku', 'umieszczonych', 'przed', 'nim', 'na', 'wysokości', 'jego', 'głowy', 'bóstw', '<STOP>']\n",
      "['średniowieczny', 'układ', 'miasta', 'zbliżony', 'do', 'owalu', 'z', 'rynkiem', 'i', 'ulicami', 'zbiegającymi', 'się', 'przy', 'dawnych', 'bramach', 'miejskich', '<STOP>']\n",
      "-----------------------------------\n",
      "['<PAD>', 'rewolucji', 'chińskiej', 'w', 'roku', 'wprowadzono', 'medycynę', 'zachodnią', 'co', 'wpłynęło', 'na', 'spadek', 'praktykowania', 'akupunktury', 'oraz', 'chińskiego', 'ziołolecznictwa', '<STOP>']\n",
      "['wiele', 'magazynów', 'm', 'in', 'metal', 'hammer', 'heavy', 'metal', 'pages', 'został', 'uznany', 'największą', 'nadzieją', 'polskiego', 'melodyjnego', 'heavy', 'metalu', '<STOP>']\n",
      "['nawet', 'gdyby', 'projekt', 'poselski', 'nie', 'miał', 'żadnych', 'walorów', 'merytorycznych', 'to', 'i', 'tak', 'uważam', 'ma', 'bardzo', 'cenną', 'zaletę', '<STOP>']\n",
      "['r', 'weszła', 'w', 'życie', 'umowa', 'podpisana', 'rok', 'wcześniej', 'w', 'baku', 'o', 'ruchu', 'bezwizowym', 'dla', 'posiadaczy', 'paszportów', 'dyplomatycznych', '<STOP>']\n",
      "['zadbali', 'aby', 'klub', 'był', 'w', 'pełni', 'profesjonalny', 'posiadał', 'swój', 'statut', 'określone', 'obowiązki', 'zadania', 'herb', 'oraz', 'hymn', 'klubowy', '<STOP>']\n",
      "-----------------------------------\n",
      "['<PAD>', 'personalna', 'rozpadła', 'się', 'w', 'roku', 'po', 'śmierci', 'króla', 'karola', 'ii', 'wittelsbacha', 'panującego', 'w', 'szwecji', 'jako', 'karol', 'xii', '<STOP>']\n",
      "['jak', 'czesto', 'sie', 'masturbowac', 'skutki', 'onanizmu', 'orgazm', 'walenie', 'seks', 'trzepac', 'darmowe', 'filmy', 'online', 'europejskie', 'dziewczyny', 'sposoby', 'masturbacji', 'chlopakow', '<STOP>']\n",
      "['w', 'dzisiejszym', 'odcinku', 'pokaże', 'wam', 'śnieżkę', 'która', 'potrafi', 'stawiać', 'pajęczyny', 'oraz', 'pająka', 'który', 'zostawia', 'za', 'sobą', 'mnóstwo', 'pajęczyn', '<STOP>']\n",
      "['finansowe', 'dla', 'budżetu', 'państwa', 'wynikające', 'z', 'wprowadzenia', 'tej', 'nowelizacji', 'można', 'podzielić', 'niejako', 'na', 'dwie', 'grupy', 'bezpośrednie', 'i', 'pośrednie', '<STOP>']\n",
      "['w', 'i', 'ogłaszano', 'konkurs', 'na', 'opracowanie', 'koncepcji', 'plastyczno', 'przestrzennej', 'ekspozycji', 'stałej', 'muzeum', 'jego', 'ostateczne', 'rozstrzygnięcie', 'nastąpiło', 'w', 'grudniu', '<STOP>']\n",
      "-----------------------------------\n",
      "['<PAD>', '<PAD>', 'tego', 'dochodzi', 'fakt', 'że', 'patenty', 'nie', 'są', 'objęte', 'przewidzianymi', 'w', 'acta', 'środkami', 'karnymi', 'i', 'środkami', 'stosowanymi', 'przy', 'kontroli', 'granicznej', '<STOP>']\n",
      "['<PAD>', 'zdawać', 'żywioły', 'moim', 'pierwszym', 'prawdziwym', 'kumplem', 'na', 'forum', 'jak', 'i', 'mistrzem', 'był', 'hipek', 'z', 'nim', 'to', 'zdawałem', 'kolejne', 'poziomy', 'katona', '<STOP>']\n",
      "['<PAD>', 'poradzić', 'się', 'kogoś', 'kto', 'doskonale', 'zna', 'się', 'na', 'takich', 'rzeczach', 'jaki', 'medykament', 'będzie', 'mógł', 'być', 'najbardziej', 'stosowny', 'do', 'wybranych', 'chorób', '<STOP>']\n",
      "['ma', 'chyba', 'takiej', 'pracy', 'gdzie', 'nie', 'byłoby', 'problemów', 'czy', 'takiego', 'kierunku', 'studiów', 'gdzie', 'jakikolwiek', 'przedmiot', 'nie', 'sprawiałby', 'nam', 'jakiś', 'większych', 'problemów', '<STOP>']\n",
      "['sto', 'lat', 'później', 'r', 'osadę', 'zamieszkiwało', 'osób', 'z', 'tego', 'aż', 'było', 'wyznania', 'ewangelickiego', 'co', 'równoznaczne', 'było', 'z', 'przynależnością', 'do', 'narodu', 'niemieckiego', '<STOP>']\n",
      "-----------------------------------\n",
      "['<PAD>', '<PAD>', '<PAD>', 'ośmiu', 'z', 'szesnastu', 'opisanych', 'dotąd', 'chorób', 'spowodowanych', 'mutacjami', 'dynamicznymi', 'przyczyną', 'choroby', 'jest', 'ekspansja', 'kodonu', 'cag', 'oznaczającego', 'aminokwas', 'glutaminę', 'w', 'sekwencji', 'kodującej', 'genu', '<STOP>']\n",
      "['<PAD>', '<PAD>', 'zostało', 'wskazane', 'na', 'wstępie', 'ingerowanie', 'w', 'czynności', 'z', 'zakresu', 'prawa', 'pracy', 'wykonywane', 'za', 'jednostki', 'będące', 'pracodawcami', 'samorządowymi', 'wykracza', 'poza', 'ustawowe', 'kompetencje', 'organu', 'nadzoru', '<STOP>']\n",
      "['<PAD>', 'było', 'także', 'instytucji', 'i', 'systemów', 'do', 'wprowadzenia', 'porównywalnych', 'z', 'krajami', 'ue', 'sposobów', 'pomocy', 'rolnictwu', 'zapewniających', 'nie', 'tylko', 'doraźną', 'ale', 'i', 'trwałą', 'poprawę', 'sytuacji', 'sektora', '<STOP>']\n",
      "['słów', 'wchodził', 'w', 'skład', 'kościół', 'powszechny', 'apostolski', 'to', 'nie', 'może', 'być', 'denominacją', 'gdyż', 'w', 'dalszym', 'ciągu', 'jest', 'tym', 'kościołem', 'powszechnym', 'katolickim', 'istniejącym', 'na', 'fundamencie', 'apostołów', '<STOP>']\n",
      "['cechą', 'wspólną', 'okazało', 'się', 'również', 'to', 'że', 'po', 'roku', 'ujawniał', 'się', 'brak', 'pokrycia', 'finansowego', 'i', 'plany', 'prędzej', 'czy', 'później', 'lądowały', 'w', 'politycznym', 'koszu', 'na', 'śmieci', '<STOP>']\n",
      "-----------------------------------\n",
      "['<PAD>', 'żą', 'po', 'pu', 'lar', 'no', 'ścią', 'wśród', 'po', 'li', 'cjan', 'tów', 'i', 'pra', 'cow', 'ni', 'ków', 'po', 'li', 'cji', 'cieszą', 'się', 'ma', 'ra', 'to', 'ny', '<STOP>']\n",
      "['zgodnie', 'z', 'art', 'ust', 'a', 'tej', 'ustawy', 'uśmiercanie', 'zwierząt', 'może', 'odbywać', 'się', 'wyłącznie', 'w', 'sposób', 'humanitarny', 'polegający', 'na', 'zadawaniu', 'przy', 'tym', 'minimum', 'cierpienia', 'fizycznego', 'i', 'psychicznego', '<STOP>']\n",
      "['propozycji', 'zawartych', 'w', 'projekcie', 'ustawy', 'o', 'podatku', 'dochodowym', 'od', 'osób', 'prawnych', 'polega', 'na', 'zastosowaniu', 'równoprawnych', 'rozwiązań', 'jak', 'w', 'przypadku', 'ustawy', 'o', 'podatku', 'dochodowym', 'od', 'osób', 'fizycznych', '<STOP>']\n",
      "['uważam', 'że', 'właśnie', 'w', 'komisji', 'edukacji', 'nauki', 'i', 'młodzieży', 'a', 'może', 'bardziej', 'w', 'podkomisji', 'młodzieży', 'powinniśmy', 'na', 'bieżąco', 'analizować', 'wydawanie', 'publicznych', 'pieniędzy', 'powinniśmy', 'analizować', 'merytoryczne', 'programy', '<STOP>']\n",
      "['sierpniu', 'został', 'dowódcą', 'armii', 'a', 'następnie', 'jeszcze', 'w', 'tym', 'samym', 'miesiącu', 'sztabu', 'generalnego', 'przy', 'dowódcy', 'frontu', 'północno', 'zachodniego', 'a', 'w', 'dniu', 'sierpnia', 'został', 'dowódcą', 'tego', 'frontu', '<STOP>']\n",
      "-----------------------------------\n",
      "['<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', 'tutaj', 'wysokiej', 'izbie', 'że', 'właśnie', 'to', 'stosunkowo', 'niewielkie', 'biuro', 'zaledwie', 'w', 'ciągu', 'miesięcy', 'było', 'w', 'stanie', 'zrealizować', 'ogromny', 'ogólnopolski', 'program', 'badawczy', 'związany', 'z', 'rocznicą', 'wprowadzenia', 'stanu', 'wojennego', '<STOP>']\n",
      "['<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', 'problem', 'i', 'musimy', 'go', 'wspólnie', 'rozwiązać', 'sprawa', 'bowiem', 'jest', 'niezwykle', 'złożona', 'i', 'ma', 'wiele', 'aspektów', 'finansowe', 'ekonomiczne', 'społeczne', 'polityczne', 'a', 'nawet', 'ideologiczne', 'czy', 'filozoficzne', 'jak', 'kto', 'woli', '<STOP>']\n",
      "['<PAD>', '<PAD>', 'pierwszych', 'tygodniach', 'kwietnia', 'br', 'rodzicom', 'dzieci', 'uczących', 'się', 'w', 'wymienionej', 'szkole', 'odczytano', 'pismo', 'podsekretarza', 'stanu', 'p', 'stanowskiego', 'a', 'adresowane', 'do', 'p', 'jana', 'borkowskiego', 'podsekretarza', 'stanu', 'ds', 'parlamentarnych', 'oraz', 'informacji', 'i', 'kultury', 'w', 'msz', '<STOP>']\n",
      "['kampanii', 'wrześniowej', 'dlek', 'raportowała', 'czołgów', 'l', 't', 'm', 'utraconych', 'wraz', 'z', 'wozami', 'dowodzenia', 'i', 'uszkodzone', 'lub', 'zepsute', 'lecz', 'ostatecznie', 'po', 'naprawach', 'straty', 'bezpowrotne', 'ograniczyły', 'się', 'do', 'czołgów', 'i', 'w', 'lutym', 'roku', 'posiadano', 'czołgów', 'na', 'stanie', '<STOP>']\n",
      "['nawet', 'w', 'artykule', 'w', 'jednej', 'z', 'gazet', 'przeczytałem', 'że', 'strata', 'rafinerii', 'trzebinia', 'z', 'tytułu', 'produkcji', 'biopaliw', 'mogłaby', 'być', 'szacowana', 'na', 'ok', 'mln', 'zł', 'a', 'straty', 'w', 'danym', 'roku', 'z', 'innej', 'działalności', 'stanowiły', 'ponad', 'mln', 'zł', '<STOP>']\n",
      "-----------------------------------\n",
      "['<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', 'interesują', 'nas', 'takie', 'usługi', 'jak', 'szkolenia', 'kierowców', 'pojazdów', 'to', 'warto', 'także', 'wziąć', 'pod', 'uwagę', 'to', 'z', 'jakimi', 'kosztami', 'i', 'ofertami', 'mamy', 'do', 'czynienia', 'a', 'na', 'pewno', 'będziemy', 'mogli', 'cieszyć', 'się', 'najlepszymi', 'efektami', 'a', 'to', 'jest', 'przecież', 'najważniejsze', '<STOP>']\n",
      "['<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', 'zostały', 'poprawki', 'zgłoszone', 'przez', 'klub', 'pis', 'z', 'których', 'jedna', 'zobowiązuje', 'ministra', 'skarbu', 'państwa', 'do', 'dostosowania', 'statutów', 'tvp', 'pr', 'i', 'jego', 'rozgłośni', 'regionalnych', 'do', 'przepisów', 'nowelizacji', 'i', 'stwierdza', 'nieważność', 'tych', 'statutowych', 'przepisów', 'które', 'z', 'tą', 'nowelizacją', 'są', 'sprzeczne', '<STOP>']\n",
      "['<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', 'wymagane', 'jest', 'spełnienie', 'przez', 'spółki', 'ubiegające', 'się', 'o', 'możliwość', 'odpłatnego', 'korzystania', 'z', 'szeregu', 'warunków', 'zgromadzenie', 'odpowiedniego', 'kapitału', 'korzystnej', 'jego', 'struktury', 'opracowania', 'realnego', 'planu', 'działania', 'itp', 'wskazujących', 'na', 'efektywne', 'działanie', 'spółki', 'oraz', 'wywiązywanie', 'się', 'z', 'zobowiązań', 'w', 'tym', 'na', 'rzecz', 'skarbu', 'państwa', '<STOP>']\n",
      "['<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', 'maju', 'r', 'wymieniona', 'szczepionka', 'przeciw', 'wzw', 'typ', 'b', 'euvax', 'b', 'została', 'wstrzymana', 'w', 'obrocie', 'na', 'terytorium', 'polskiej', 'decyzją', 'głównego', 'inspektora', 'na', 'wniosek', 'prezesa', 'urzędu', 'rejestracji', 'produktów', 'leczniczych', 'wyrobów', 'medycznych', 'i', 'produktów', 'biobójczych', 'w', 'związku', 'ze', 'zgłoszeniami', 'niepożądanych', 'odczynów', 'poszczepiennych', 'związanych', 'z', 'podaniem', 'przedmiotowej', 'szczepionki', '<STOP>']\n",
      "['w', 'sferze', 'oświaty', 'działania', 'placówek', 'dyplomatyczno', 'konsularnych', 'obejmują', 'm', 'in', 'pomoc', 'w', 'zakładaniu', 'punktów', 'nauczania', 'języka', 'polskiego', 'fundowanie', 'nagród', 'dla', 'najlepszych', 'uczniów', 'i', 'nauczycieli', 'czy', 'współdziałanie', 'w', 'realizacji', 'przedsięwzięć', 'oświatowych', 'inicjowanych', 'przez', 'podmioty', 'krajowe', 'jak', 'na', 'przykład', 'udział', 'w', 'prowadzeniu', 'rekrutacji', 'studentów', 'polskiego', 'pochodzenia', 'organizowanie', 'przyjazdów', 'nauczycieli', 'polonijnych', 'na', 'kursy', 'do', 'kraju', 'przeprowadzanie', 'eliminacji', 'do', 'konkursów', 'i', 'olimpiad', 'języka', 'polskiego', '<STOP>']\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "for x,y in loader:\n",
    "    for s_in, s_out in zip(x,y):\n",
    "        print(dataset.indexes_to_sentence([x.item() for x in s_out]))\n",
    "    print('-----------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(1171, 100, padding_idx=0)\n",
       "  (rnn): RNN(100, 512, num_layers=2, batch_first=True)\n",
       "  (fc): Linear(in_features=512, out_features=1171, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, dataset, device, embedding_dim=100, hidden_size = 512, num_layers = 2):\n",
    "        super(RNN, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        n_vocab = len(dataset.unique_words)\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=n_vocab,\n",
    "            embedding_dim=embedding_dim,\n",
    "            padding_idx=0\n",
    "        )\n",
    "\n",
    "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, n_vocab)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x, h0 = None):\n",
    "\n",
    "        x.to(self.device)\n",
    "\n",
    "        embed = self.embedding(x)\n",
    "\n",
    "        if h0 == None:\n",
    "            h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size, device=self.device)\n",
    "\n",
    "        output, state = self.rnn(embed, h0)\n",
    "        \n",
    "        logits = self.fc(output)\n",
    "\n",
    "        return logits, state\n",
    "\n",
    "    def predict(self, x):\n",
    "        logits, state = self.forward(x)\n",
    "        return self.softmax(x), state\n",
    "\n",
    "        \n",
    "model = RNN(dataset, device) \n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('./models/RNN_30ep.model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'batch': 9, 'loss': 3.664839029312134}\n",
      "{'epoch': 1, 'batch': 9, 'loss': 1.8237115144729614}\n",
      "{'epoch': 2, 'batch': 9, 'loss': 0.39195358753204346}\n",
      "{'epoch': 3, 'batch': 9, 'loss': 4.460778713226318}\n",
      "{'epoch': 4, 'batch': 9, 'loss': 1.3296624422073364}\n",
      "{'epoch': 5, 'batch': 9, 'loss': 0.24708372354507446}\n",
      "{'epoch': 6, 'batch': 9, 'loss': 0.12662452459335327}\n",
      "{'epoch': 7, 'batch': 9, 'loss': 0.057946451008319855}\n",
      "{'epoch': 8, 'batch': 9, 'loss': 0.037013374269008636}\n",
      "{'epoch': 9, 'batch': 9, 'loss': 0.04206681624054909}\n",
      "{'epoch': 10, 'batch': 9, 'loss': 0.05053024739027023}\n",
      "{'epoch': 11, 'batch': 9, 'loss': 0.038840748369693756}\n",
      "{'epoch': 12, 'batch': 9, 'loss': 0.030022824183106422}\n",
      "{'epoch': 13, 'batch': 9, 'loss': 0.01380565483123064}\n",
      "{'epoch': 14, 'batch': 9, 'loss': 0.011321687139570713}\n",
      "{'epoch': 15, 'batch': 9, 'loss': 0.009189214557409286}\n",
      "{'epoch': 16, 'batch': 9, 'loss': 0.007803468033671379}\n",
      "{'epoch': 17, 'batch': 9, 'loss': 0.0068697272799909115}\n",
      "{'epoch': 18, 'batch': 9, 'loss': 0.00616737874224782}\n",
      "{'epoch': 19, 'batch': 9, 'loss': 0.005612153559923172}\n",
      "{'epoch': 20, 'batch': 9, 'loss': 0.005163012072443962}\n",
      "{'epoch': 21, 'batch': 9, 'loss': 0.004790528677403927}\n",
      "{'epoch': 22, 'batch': 9, 'loss': 0.0044739339500665665}\n",
      "{'epoch': 23, 'batch': 9, 'loss': 0.004199483897536993}\n",
      "{'epoch': 24, 'batch': 9, 'loss': 0.003957943059504032}\n",
      "{'epoch': 25, 'batch': 9, 'loss': 0.0037428836803883314}\n",
      "{'epoch': 26, 'batch': 9, 'loss': 0.003549756482243538}\n",
      "{'epoch': 27, 'batch': 9, 'loss': 0.0033750291913747787}\n",
      "{'epoch': 28, 'batch': 9, 'loss': 0.0032159932889044285}\n",
      "{'epoch': 29, 'batch': 9, 'loss': 0.003070499747991562}\n"
     ]
    }
   ],
   "source": [
    "def train(dataset, model, max_epochs = 30, batch_size = 10):\n",
    "    model.train()\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, collate_fn=pad_collate)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(max_epochs):        \n",
    "        for batch, (x, y) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred, _ = model(x)\n",
    "            loss = criterion(y_pred.transpose(1, 2), y)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print({ 'epoch': epoch, 'batch': batch, 'loss': loss.item() })\n",
    "            \n",
    "train(dataset, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), './models/RNN_60ep.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dataset, model, text, next_words=100):\n",
    "    model.eval()\n",
    "\n",
    "    words = text.split(' ')\n",
    "    \n",
    "    for i in range(0, next_words):\n",
    "        x = torch.tensor([[dataset.word_to_index[w] for w in words]], device=model.device)\n",
    "        y_pred, _ = model(x)\n",
    "\n",
    "        # print(y_pred)\n",
    "\n",
    "        last_word_logits = y_pred[0][-1]\n",
    "        p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().cpu().numpy()\n",
    "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
    "        words.append(dataset.index_to_word[word_index])\n",
    "\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['zmienia',\n",
       " 'się',\n",
       " 'również',\n",
       " 'klimat',\n",
       " 'całej',\n",
       " 'planety',\n",
       " 'ponieważ',\n",
       " 'lasy',\n",
       " 'tropikalne',\n",
       " 'są',\n",
       " 'ważnym',\n",
       " 'ogniwem',\n",
       " 'obiegu',\n",
       " 'wielu',\n",
       " 'pierwiastków',\n",
       " '<STOP>',\n",
       " 'raportowała',\n",
       " 'czołgów']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(dataset, model, \"zmienia się również\", next_words=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_2(dataset, model, text, next_words=100):\n",
    "    model.eval()\n",
    "\n",
    "    words = text.split(' ')\n",
    "\n",
    "    x = torch.tensor([[dataset.word_to_index[w] for w in words]], device=model.device)\n",
    "    y_pred, hidden_state = model(x)\n",
    "    \n",
    "    for i in range(0, next_words):\n",
    "        last_word_logits = y_pred[0][-1]\n",
    "        p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().cpu().numpy()\n",
    "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
    "        words.append(dataset.index_to_word[word_index])\n",
    "\n",
    "        y_pred, hidden_state = model(torch.tensor([[word_index]], device=model.device), hidden_state)\n",
    "\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['świadkowie',\n",
       " 'zdarzenia',\n",
       " 'sfotografowali',\n",
       " 'sprawcę',\n",
       " 'podczas',\n",
       " 'gwałtu',\n",
       " 'i',\n",
       " 'powiadomili',\n",
       " 'policję',\n",
       " 'która',\n",
       " 'schwytała',\n",
       " 'go',\n",
       " 'kilka',\n",
       " 'godzin',\n",
       " 'później',\n",
       " '<STOP>']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_2(dataset, model, \"świadkowie\", next_words=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def best_logits(logits, n):\n",
    "\n",
    "\n",
    "def beam_search(dataset, model, text, max_next_words, n_solutions):\n",
    "    model.eval()\n",
    "\n",
    "    words = text.split(' ')\n",
    "\n",
    "    x = torch.tensor([[dataset.word_to_index[w] for w in words]], device=model.device)\n",
    "\n",
    "    y_pred, hidden_state = model(x)\n",
    "    last_word_logits = y_pred[0][-1]\n",
    "    log_p = torch.nn.functional.log_softmax(last_word_logits, dim=0).detach().cpu().numpy()\n",
    "\n",
    "    best_indices = np.argsort(log_p)[::-1][:n_solutions]\n",
    "\n",
    "    solutions = [([index], log_p[index], hidden_state) for index in best_indices]\n",
    "\n",
    "    for i in range(1, max_next_words):\n",
    "        new_solutions = []\n",
    "\n",
    "        for (prefix, score, prefix_state) in solutions:\n",
    "            x = torch.tensor([[prefix[-1]]], device=model.device)\n",
    "            y_pred, hi = model(x, prefix_state)\n",
    "            last_word_logits = y_pred[0][-1]\n",
    "            log_p = torch.nn.functional.log_softmax(last_word_logits, dim=0).detach().cpu().numpy()\n",
    "            best_indices = np.argsort(log_p)[::-1][:n_solutions]\n",
    "            new_solutions += [(prefix + [ind], score + log_p[ind], hi) for ind in best_indices]\n",
    "\n",
    "        best_indices = np.argsort([score for (_, score, _) in new_solutions])[::-1][:n_solutions]\n",
    "\n",
    "        solutions = [new_solutions[ind] for ind in best_indices]\n",
    "\n",
    "    return [([dataset.index_to_word[w] for w in sent], lp) for (sent, lp, _) in solutions]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1107\n",
      "281\n",
      "0\n",
      "803\n",
      "347\n",
      "1044\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(['zdarzenia', 'sfotografowali', 'sprawcę'], -0.000649821),\n",
       " (['zdarzenia', 'sfotografowali', 'w'], -8.26417),\n",
       " (['zdarzenia', 'sfotografowali', 'uczniów'], -9.916093)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beam_search(dataset, model, \"świadkowie\", max_next_words=3, n_solutions=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
