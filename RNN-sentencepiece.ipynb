{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EOL0AvaDFeQP"
      },
      "outputs": [],
      "source": [
        "import gensim.utils as utils\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from nltk.tokenize import word_tokenize\n",
        "import sentencepiece as spm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=./data/train_test_small.txt --vocab_size=10000 --model_prefix=vs10k --pad_id=0 --unk_id=1 --bos_id=2 --eos_id=3\n",
            "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: ./data/train_test_small.txt\n",
            "  input_format: \n",
            "  model_prefix: vs10k\n",
            "  model_type: UNIGRAM\n",
            "  vocab_size: 10000\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 0\n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  hard_vocab_limit: 1\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 1\n",
            "  bos_id: 2\n",
            "  eos_id: 3\n",
            "  pad_id: 0\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(178) LOG(INFO) Loading corpus: ./data/train_test_small.txt\n",
            "trainer_interface.cc(140) LOG(INFO) Loaded 1000000 lines\n",
            "trainer_interface.cc(117) LOG(WARNING) Too many sentences are loaded! (1100000), which may slow down training.\n",
            "trainer_interface.cc(119) LOG(WARNING) Consider using --input_sentence_size=<size> and --shuffle_input_sentence=true.\n",
            "trainer_interface.cc(122) LOG(WARNING) They allow to randomly sample <size> sentences from the entire corpus.\n",
            "trainer_interface.cc(385) LOG(INFO) Loaded all 1100000 sentences\n",
            "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <pad>\n",
            "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(466) LOG(INFO) all chars count=136087851\n",
            "trainer_interface.cc(477) LOG(INFO) Done: 99.9582% characters are covered.\n",
            "trainer_interface.cc(487) LOG(INFO) Alphabet size=77\n",
            "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.999582\n",
            "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 1100000 sentences.\n",
            "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
            "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
            "unigram_model_trainer.cc(194) LOG(INFO) Initialized 1000000 seed sentencepieces\n",
            "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 1100000\n",
            "trainer_interface.cc(537) LOG(INFO) Done! 1193298\n",
            "unigram_model_trainer.cc(489) LOG(INFO) Using 1193298 sentences for EM training\n",
            "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=431722 obj=13.4883 num_tokens=2688490 num_tokens/piece=6.22736\n",
            "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=368076 obj=10.6863 num_tokens=2688840 num_tokens/piece=7.30512\n",
            "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=276030 obj=10.6687 num_tokens=2760986 num_tokens/piece=10.0025\n",
            "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=275890 obj=10.655 num_tokens=2762331 num_tokens/piece=10.0124\n",
            "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=206917 obj=10.7251 num_tokens=2905940 num_tokens/piece=14.044\n",
            "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=206916 obj=10.7053 num_tokens=2905996 num_tokens/piece=14.0443\n",
            "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=155187 obj=10.8242 num_tokens=3070956 num_tokens/piece=19.7887\n",
            "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=155186 obj=10.7983 num_tokens=3071112 num_tokens/piece=19.7899\n",
            "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=116389 obj=10.9499 num_tokens=3242083 num_tokens/piece=27.8556\n",
            "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=116389 obj=10.9165 num_tokens=3242151 num_tokens/piece=27.8562\n",
            "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=87291 obj=11.1002 num_tokens=3414183 num_tokens/piece=39.1127\n",
            "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=87291 obj=11.063 num_tokens=3414642 num_tokens/piece=39.1179\n",
            "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=65468 obj=11.2793 num_tokens=3587122 num_tokens/piece=54.792\n",
            "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=65468 obj=11.2358 num_tokens=3587504 num_tokens/piece=54.7978\n",
            "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=49101 obj=11.4885 num_tokens=3758679 num_tokens/piece=76.5499\n",
            "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=49101 obj=11.4391 num_tokens=3759211 num_tokens/piece=76.5608\n",
            "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=36825 obj=11.73 num_tokens=3934809 num_tokens/piece=106.852\n",
            "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=36825 obj=11.6726 num_tokens=3935427 num_tokens/piece=106.868\n",
            "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=27618 obj=11.999 num_tokens=4114695 num_tokens/piece=148.986\n",
            "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=27618 obj=11.9337 num_tokens=4115552 num_tokens/piece=149.017\n",
            "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=20713 obj=12.3017 num_tokens=4296416 num_tokens/piece=207.426\n",
            "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=20713 obj=12.2287 num_tokens=4297013 num_tokens/piece=207.455\n",
            "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=15534 obj=12.6308 num_tokens=4479950 num_tokens/piece=288.396\n",
            "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=15534 obj=12.5512 num_tokens=4480617 num_tokens/piece=288.439\n",
            "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=11650 obj=13.0009 num_tokens=4660876 num_tokens/piece=400.075\n",
            "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=11650 obj=12.9129 num_tokens=4661661 num_tokens/piece=400.143\n",
            "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=11000 obj=12.9977 num_tokens=4706491 num_tokens/piece=427.863\n",
            "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=11000 obj=12.9794 num_tokens=4706962 num_tokens/piece=427.906\n",
            "trainer_interface.cc(615) LOG(INFO) Saving model: vs10k.model\n",
            "trainer_interface.cc(626) LOG(INFO) Saving vocabs: vs10k.vocab\n"
          ]
        }
      ],
      "source": [
        "# spm.SentencePieceTrainer.train('--input=./data/train_test_small.txt --model_prefix=tokenization --vocab_size=2000')\n",
        "spm.SentencePieceTrainer.train('--input=./data/train_test_small.txt --vocab_size=10000 --model_prefix=vs10k --pad_id=0 --unk_id=1 --bos_id=2 --eos_id=3')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['▁Z', 'C', 'ar', 'r', '▁po', 'rzucił', '▁pracę', '▁księg', 'owego', '▁w', '▁1983', '▁i', '▁u', 'tworzy', 'ł', '▁swoją', '▁pierwszą', '▁klin', 'i', 'kę', '▁E', 'a', 'sy', 'wa', 'y', ',', '▁aby', '▁pomaga', 'ć', '▁innym', '▁pal', 'a', 'czo', 'm', '.']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<bound method SentencePieceProcessor.Decode of <sentencepiece.SentencePieceProcessor; proxy of <Swig Object of type 'sentencepiece::SentencePieceProcessor *' at 0x7fa60cac6cc0> >>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sp = spm.SentencePieceProcessor(\"m.model\")\n",
        "pieces = sp.encode_as_pieces('ZCarr porzucił pracę księgowego w 1983 i utworzył swoją pierwszą klinikę Easyway, aby pomagać innym palaczom.')\n",
        "print(pieces)\n",
        "sp.decode_pieces(pieces)\n",
        "\n",
        "ids = sp.encode_as_ids('ZCarr porzucił pracę księgowego w 1983 i utworzył swoją pierwszą klinikę Easyway, aby pomagać innym palaczom.')\n",
        "\n",
        "sp.encode_as_ids\n",
        "\n",
        "# print(sp.piece_to_id())\n",
        "sp.id_to_piece([5,6,7])\n",
        "# sp.decode_ids(ids + [sp.eos_id()])\n",
        "sp.vocab_size()\n",
        "\n",
        "sp.decode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SentencePieceDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, sp_model, corpus_path = './data/train_small.txt', corpus_length = None, device = None):\n",
        "        corpus_file = open(corpus_path)\n",
        "\n",
        "        self.sp_processor = spm.SentencePieceProcessor(sp_model)\n",
        "\n",
        "        self.vocab_size = self.sp_processor.vocab_size()\n",
        "\n",
        "        if device == None:\n",
        "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "        if corpus_length == None:\n",
        "            corpus_length = sum(1 for line in corpus_file)\n",
        "            corpus_file.seek(0)\n",
        "        \n",
        "        self.corpus_indexes = []\n",
        "\n",
        "\n",
        "        for i in tqdm(range(corpus_length)):\n",
        "            self.corpus_indexes.append(self.sp_processor.encode_as_ids(corpus_file.readline()))\n",
        "\n",
        "        self.corpus_indexes = sorted(self.corpus_indexes, key=lambda x: len(x))\n",
        "\n",
        "        self.input_corpus_indexes = [[self.sp_processor.bos_id()] + corp_ind for corp_ind in self.corpus_indexes]\n",
        "\n",
        "        self.output_corpus_indexes = [corp_ind + [self.sp_processor.eos_id()] for corp_ind in self.corpus_indexes]\n",
        "        \n",
        "    def ids_to_pieces(self, ids):\n",
        "        return [self.sp_processor.id_to_piece(id) for id in ids]\n",
        "\n",
        "    def pieces_to_ids(self, pieces):\n",
        "        return [self.sp_processor.piece_to_id(piece) for piece in pieces]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_corpus_indexes)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return (torch.tensor(self.input_corpus_indexes[index], device=self.device),\n",
        "            torch.tensor(self.output_corpus_indexes[index], device=self.device))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbZkeW_cFeQW",
        "outputId": "2599f145-3929-4ecf-e35c-81df70d0e0cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU is available\n"
          ]
        }
      ],
      "source": [
        "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
        "is_cuda = torch.cuda.is_available()\n",
        "\n",
        "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU not available, CPU used\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "qDc7AlZEFeQ0"
      },
      "outputs": [],
      "source": [
        "def pad_collate(data):\n",
        "    def left_pad_sequence(tensors):\n",
        "        max_len = max(list(map(len, tensors)))\n",
        "        padded_seq = [torch.hstack([torch.zeros(max_len - len(t), device=t.device, dtype=torch.int32), t]) for t in tensors]\n",
        "        return torch.stack(padded_seq)\n",
        "\n",
        "\n",
        "    inputs = [d[0] for d in data]\n",
        "    outputs = [d[1] for d in data]\n",
        "    inputs = left_pad_sequence(inputs)\n",
        "    outputs = left_pad_sequence(outputs)\n",
        "    return inputs, outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POBTPy_0FeQ2",
        "outputId": "2a11783e-ea76-4c72-c523-0f57a4c4e2e0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000000/1000000 [00:38<00:00, 26250.62it/s]\n"
          ]
        }
      ],
      "source": [
        "dataset = SentencePieceDataset(sp_model = \"./m.model\", corpus_length = None, device=device)\n",
        "# loader = torch.utils.data.DataLoader(dataset, batch_size=20, collate_fn=pad_collate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100000/100000 [00:02<00:00, 38607.64it/s]\n"
          ]
        }
      ],
      "source": [
        "test_dataset = SentencePieceDataset(sp_model = \"./m.model\", corpus_path = \"./data/test_small.txt\", corpus_length = None, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "W0zGnugNFeQ3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['<pad>', '<pad>', '<s>', '▁Kto', '▁wstrzymał', '▁się', '▁od', '▁głosu', '?']\n",
            "['<pad>', '<pad>', '▁Kto', '▁wstrzymał', '▁się', '▁od', '▁głosu', '?', '</s>']\n",
            "['<pad>', '<pad>', '<s>', '▁Kto', '▁wstrzymał', '▁się', '▁od', '▁głosowania', '?']\n",
            "['<pad>', '<pad>', '▁Kto', '▁wstrzymał', '▁się', '▁od', '▁głosowania', '?', '</s>']\n",
            "['<pad>', '<pad>', '<s>', '▁W', '▁braku', '▁odpowiedzi', '▁powtarza', '▁pytanie', '.']\n",
            "['<pad>', '<pad>', '▁W', '▁braku', '▁odpowiedzi', '▁powtarza', '▁pytanie', '.', '</s>']\n",
            "['<pad>', '<pad>', '<s>', '▁Warto', '▁zauważyć', '▁zmianę', '▁stanowiska', '▁rządu', '.']\n",
            "['<pad>', '<pad>', '▁Warto', '▁zauważyć', '▁zmianę', '▁stanowiska', '▁rządu', '.', '</s>']\n",
            "['<pad>', '<pad>', '<s>', '▁A', '▁dlaczego', '▁akurat', '▁50', '▁g', '?']\n",
            "['<pad>', '<pad>', '▁A', '▁dlaczego', '▁akurat', '▁50', '▁g', '?', '</s>']\n",
            "['<pad>', '<s>', '▁Otóż', '▁jaką', '▁wiz', 'ję', '▁ma', '▁prezydent', '?']\n",
            "['<pad>', '▁Otóż', '▁jaką', '▁wiz', 'ję', '▁ma', '▁prezydent', '?', '</s>']\n",
            "['<pad>', '<s>', '▁W', '▁2007', '▁roku', '▁zakończył', '▁piłkarską', '▁karierę', '.']\n",
            "['<pad>', '▁W', '▁2007', '▁roku', '▁zakończył', '▁piłkarską', '▁karierę', '.', '</s>']\n",
            "['<pad>', '<s>', '▁Zespół', '▁zakończył', '▁działalność', '▁w', '▁1997', '▁roku', '.']\n",
            "['<pad>', '▁Zespół', '▁zakończył', '▁działalność', '▁w', '▁1997', '▁roku', '.', '</s>']\n",
            "['<pad>', '<s>', '▁Później', '▁wycofa', 'ł', '▁się', '▁z', '▁polityki', '.']\n",
            "['<pad>', '▁Później', '▁wycofa', 'ł', '▁się', '▁z', '▁polityki', '.', '</s>']\n",
            "['<pad>', '<s>', '▁Co', '▁się', '▁okazuje', ',', '▁Wysoka', '▁Izbo', '?']\n",
            "['<pad>', '▁Co', '▁się', '▁okazuje', ',', '▁Wysoka', '▁Izbo', '?', '</s>']\n",
            "['<pad>', '<s>', '▁Odkryt', 'a', '▁10', '▁marca', '▁2002', '▁roku', '.']\n",
            "['<pad>', '▁Odkryt', 'a', '▁10', '▁marca', '▁2002', '▁roku', '.', '</s>']\n",
            "['<pad>', '<s>', '▁Komisja', '▁wnosi', '▁o', '▁przyjęcie', '▁tej', '▁poprawki', '.']\n",
            "['<pad>', '▁Komisja', '▁wnosi', '▁o', '▁przyjęcie', '▁tej', '▁poprawki', '.', '</s>']\n",
            "['<s>', '▁Warszawa', ',', '▁dnia', '▁10', '▁lipca', '▁2014', '▁r', '.']\n",
            "['▁Warszawa', ',', '▁dnia', '▁10', '▁lipca', '▁2014', '▁r', '.', '</s>']\n",
            "['<s>', '▁To', '▁jest', '▁już', '▁jasne', ',', '▁panie', '▁pośle', '.']\n",
            "['▁To', '▁jest', '▁już', '▁jasne', ',', '▁panie', '▁pośle', '.', '</s>']\n",
            "['<s>', '▁Komisje', '▁w', 'noszą', '▁o', '▁przyjęcie', '▁tej', '▁poprawki', '.']\n",
            "['▁Komisje', '▁w', 'noszą', '▁o', '▁przyjęcie', '▁tej', '▁poprawki', '.', '</s>']\n",
            "['<s>', '▁Komisje', '▁w', 'noszą', '▁o', '▁odrzucenie', '▁tej', '▁poprawki', '.']\n",
            "['▁Komisje', '▁w', 'noszą', '▁o', '▁odrzucenie', '▁tej', '▁poprawki', '.', '</s>']\n",
            "['<s>', '▁Warszawa', ',', '▁dnia', '▁15', '▁maja', '▁2002', '▁r', '.']\n",
            "['▁Warszawa', ',', '▁dnia', '▁15', '▁maja', '▁2002', '▁r', '.', '</s>']\n",
            "['<s>', '▁Warszawa', ',', '▁dnia', '▁29', '▁czerwca', '▁2012', '▁r', '.']\n",
            "['▁Warszawa', ',', '▁dnia', '▁29', '▁czerwca', '▁2012', '▁r', '.', '</s>']\n",
            "['<s>', '▁Stwierdzam', ',', '▁że', '▁Sejm', '▁wniosek', '▁mniejszości', '▁odrzucił', '.']\n",
            "['▁Stwierdzam', ',', '▁że', '▁Sejm', '▁wniosek', '▁mniejszości', '▁odrzucił', '.', '</s>']\n",
            "['<s>', '▁Myślę', ',', '▁że', '▁co', '▁do', '▁innych', '▁spraw', '...']\n",
            "['▁Myślę', ',', '▁że', '▁co', '▁do', '▁innych', '▁spraw', '...', '</s>']\n",
            "-----------------------------------\n"
          ]
        }
      ],
      "source": [
        "for x,y in loader:\n",
        "    for s_in, s_out in zip(x,y):\n",
        "        print(dataset.ids_to_pieces([x.item() for x in s_in]))\n",
        "        print(dataset.ids_to_pieces([x.item() for x in s_out]))\n",
        "    print('-----------------------------------')\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xGVwZGZFeQ6",
        "outputId": "274ee721-7ace-45ca-fc40-137071536057"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (embedding): Embedding(8000, 100, padding_idx=0)\n",
              "  (rnn): RNN(100, 128, num_layers=2, batch_first=True)\n",
              "  (fc): Linear(in_features=128, out_features=8000, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, dataset, device, embedding_dim=100, hidden_size = 128, num_layers = 2):\n",
        "        super(RNN, self).__init__()\n",
        "        self.device = device\n",
        "\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "        n_vocab = dataset.vocab_size\n",
        "\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=n_vocab,\n",
        "            embedding_dim=embedding_dim,\n",
        "            padding_idx=0\n",
        "        )\n",
        "\n",
        "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size, n_vocab)\n",
        "\n",
        "    def forward(self, x, h0 = None):\n",
        "\n",
        "        x.to(self.device)\n",
        "\n",
        "        embed = self.embedding(x)\n",
        "\n",
        "        if h0 == None:\n",
        "            if len(x.shape) == 2:\n",
        "                h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size, device=self.device)\n",
        "            else:\n",
        "                h0 = torch.zeros(self.num_layers, self.hidden_size, device = self.device)\n",
        "\n",
        "        output, state = self.rnn(embed, h0)\n",
        "        \n",
        "        logits = self.fc(output)\n",
        "\n",
        "        return logits, state\n",
        "\n",
        "\n",
        "        \n",
        "model = RNN(dataset, device) \n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "sqZKAdjwFeQ8"
      },
      "outputs": [],
      "source": [
        "# model.load_state_dict(torch.load('./models/RNN_30ep.model'))\n",
        "model.load_state_dict(torch.load(\"./models/RNN_1kk_bs50_2ep.model\"))\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6KwXbBbFeQ8",
        "outputId": "52d55caa-2ba3-44bf-cbc4-7760d25d96ab"
      },
      "outputs": [],
      "source": [
        "def train(dataset, model, max_epochs = 30, batch_size = 1):\n",
        "    model.train()\n",
        "\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, collate_fn=pad_collate)\n",
        "    \n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    for epoch in range(2, max_epochs):        \n",
        "        for batch, (x, y) in enumerate(dataloader):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            y_pred, _ = model(x)\n",
        "            loss = criterion(y_pred.transpose(1, 2), y)\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if (batch+1) % 500 == 0:\n",
        "                print({ 'epoch': epoch, 'batch': batch + 1, 'loss': loss.item() })\n",
        "                # torch.cuda.empty_cache()\n",
        "        \n",
        "        torch.save(model.state_dict(), f\"./models/RNN_1kk_bs{batch_size}_{epoch+1}ep.model\")\n",
        "            \n",
        "train(dataset, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "k4FjhkcOFeQ9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.load_state_dict(torch.load(\"./models/RNN/RNN_1kk_bs20_30ep.model\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "lw-CThdoFeQ_"
      },
      "outputs": [],
      "source": [
        "def predict(dataset, model, text, next_words=100):\n",
        "    model.eval()\n",
        "\n",
        "    \n",
        "    ids = dataset.sp_processor.encode_as_ids(text)\n",
        "\n",
        "    print(ids)\n",
        "\n",
        "    x = torch.tensor([ids], device=model.device)\n",
        "    y_pred, hidden_state = model(x)\n",
        "    \n",
        "    for i in range(0, next_words):\n",
        "        last_word_logits = y_pred[0][-1]\n",
        "        p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().cpu().numpy()\n",
        "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
        "        # word_index = np.argmax(p)\n",
        "        ids.append(int(word_index))\n",
        "\n",
        "        y_pred, hidden_state = model(torch.tensor([[word_index]], device=model.device), hidden_state)\n",
        "\n",
        "    print(dataset.ids_to_pieces(ids))\n",
        "    return dataset.sp_processor.decode_ids(ids)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "_6PYBzHSFeQ_",
        "outputId": "01f09b3e-c4bb-4727-f882-6e7f448d4179"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[4812, 4360, 5182, 1164, 1015]\n",
            "['▁Jednakże', '▁największe', '▁znaczenia', '▁zawsze', '▁miała', '▁bowiem', '▁do', '▁sytuacji', ',', '▁na', '▁którym', '▁umieszczono', '▁ze', '▁sobą', '▁opodatk']\n",
            "Jednakże największe znaczenia zawsze miała bowiem do sytuacji, na którym umieszczono ze sobą opodatk\n"
          ]
        }
      ],
      "source": [
        "for _ in range(1):\n",
        "    print(predict(dataset, model, \"Jednakże największe znaczenia zawsze miała\", next_words=10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "7s4nqL3DFeRA"
      },
      "outputs": [],
      "source": [
        "def beam_search(dataset, model, text, max_next_words, n_solutions):\n",
        "    model.eval()\n",
        "\n",
        "    words = text.split(' ')\n",
        "\n",
        "    ids = dataset.sp_processor.encode_as_ids(text)\n",
        "\n",
        "    x = torch.tensor([ids], device=model.device)\n",
        "\n",
        "    y_pred, hidden_state = model(x)\n",
        "    last_word_logits = y_pred[0][-1]\n",
        "    log_p = torch.nn.functional.log_softmax(last_word_logits, dim=0).detach().cpu().numpy()\n",
        "\n",
        "    best_indices = np.argsort(log_p)[::-1][:n_solutions]\n",
        "\n",
        "    solutions = [(list(map(int, ids)) + [int(index)], log_p[index], hidden_state) for index in best_indices]\n",
        "\n",
        "    for i in range(1, max_next_words):\n",
        "        new_solutions = []\n",
        "\n",
        "        for (prefix, score, prefix_state) in solutions:\n",
        "            x = torch.tensor([[prefix[-1]]], device=model.device)\n",
        "            y_pred, hi = model(x, prefix_state)\n",
        "            last_word_logits = y_pred[0][-1]\n",
        "            log_p = torch.nn.functional.log_softmax(last_word_logits, dim=0).detach().cpu().numpy()\n",
        "            best_indices = np.argsort(log_p)[::-1][:n_solutions]\n",
        "            new_solutions += [(prefix + [int(ind)], score + log_p[ind], hi) for ind in best_indices]\n",
        "\n",
        "        best_indices = np.argsort([score for (_, score, _) in new_solutions])[::-1][:n_solutions]\n",
        "\n",
        "        solutions = [new_solutions[ind] for ind in best_indices]\n",
        "\n",
        "    return [(dataset.sp_processor.decode_ids(sent), lp) for (sent, lp, _) in solutions]\n",
        "    # return solutions\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "QOcZ-ARwFeRB",
        "outputId": "1cf614d6-1ad1-49f4-ad2f-29f4e124ec55"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('świadkowie zastępczej,', -5.503442),\n",
              " ('świadkowie, w tym', -6.339065),\n",
              " ('świadkowie publicznego, w', -6.4336476)]"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "beam_search(dataset, model, \"świadkowie\", max_next_words=3, n_solutions=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "dDxo3-GrFeRB"
      },
      "outputs": [],
      "source": [
        "def perplexity(dataset, model):\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        criterion = nn.CrossEntropyLoss(reduction='sum')\n",
        "\n",
        "        logs_sum = 0\n",
        "        n_samples = 0\n",
        "        \n",
        "        for x,y in dataset:\n",
        "            y_pred, _ = model(x)\n",
        "            n_samples += len(x)\n",
        "            logs_sum  += criterion(y_pred, y)\n",
        "\n",
        "        return np.exp(logs_sum.item() / n_samples)\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(16469591., device='cuda:0')\n",
            "3251844\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "158.3318002587777"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "perplexity(test_dataset, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6401e+08, device='cuda:0')\n",
            "32565275\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "153.89469996251682"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "perplexity(dataset, model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "RNN.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
